<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>OPTIMIZATIONS for Machine Learning</title>
    <description>A little blog collecting optimization methods in machine learning field</description>
    <link>http://www.quxiaofeng.me/optimization/</link>
    <atom:link href="http://www.quxiaofeng.me/optimization/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 24 Jul 2015 14:08:40 +0800</pubDate>
    <lastBuildDate>Fri, 24 Jul 2015 14:08:40 +0800</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>Kernel and Kernel: Reproducing Kernel Hilbert Space and Kernel Method</title>
        <description>&lt;h2 id=&quot;kernel&quot;&gt;Kernel&lt;/h2&gt;

&lt;p&gt;再生核的定义&lt;/p&gt;

&lt;p&gt;Definition. &lt;script type=&quot;math/tex&quot;&gt;k: \mathcal{X} \times \mathcal{X} \rightarrow \Re &lt;/script&gt; is a kernel if&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;核函数 &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;  对称 isymmetric: &lt;script type=&quot;math/tex&quot;&gt;k(x,y)=k(y,x)&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;核函数 &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; 半正定 positive semi-definite&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;i.e., &lt;script type=&quot;math/tex&quot;&gt;\forall x_1, x_2, \ldots , x_n \in \mathcal{X}&lt;/script&gt;, the “Gram Matrix” &lt;script type=&quot;math/tex&quot;&gt;K&lt;/script&gt; defined by &lt;script type=&quot;math/tex&quot;&gt;K_{ij} =  k(x_i,x_j)&lt;/script&gt; is positive semi-definite. (A matrix &lt;script type=&quot;math/tex&quot;&gt;M \in \Re^{n \times n}&lt;/script&gt; is positive semi-definite if &lt;script type=&quot;math/tex&quot;&gt;\forall a \in \Re^n&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;a&#39; M a \ge 0&lt;/script&gt;.)&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;再生核希尔伯特空间是从低维数据到函数泛函映射。Hilbert 空间“Frechet-Riesz”表现定理。&lt;/p&gt;

&lt;p&gt;希尔伯特空间是定义了内积的空间；相对应的 Banach 是定义了范数的空间。&lt;/p&gt;

&lt;h2 id=&quot;reproducing-kernel-feature-map&quot;&gt;Reproducing Kernel Feature Map&lt;/h2&gt;

&lt;p&gt;再生核特征映射&lt;br /&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Phi_x(\cdot) \triangleq k(\cdot,x) &lt;/script&gt;&lt;/p&gt;

&lt;p&gt;即对任意线性泛函&lt;script type=&quot;math/tex&quot;&gt; \Phi(\cdot) &lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt; \exists x_\Phi \in H &lt;/script&gt;, 使得 &lt;script type=&quot;math/tex&quot;&gt; \Phi(\cdot) = (\cdot, x_\Phi) &lt;/script&gt;&lt;/p&gt;

&lt;p&gt;To be continued…&lt;/p&gt;
</description>
        <pubDate>Sun, 19 Jul 2015 17:22:53 +0800</pubDate>
        <link>http://www.quxiaofeng.me/optimization/2015/07/19/kernel-and-kenel/</link>
        <guid isPermaLink="true">http://www.quxiaofeng.me/optimization/2015/07/19/kernel-and-kenel/</guid>
        
        
      </item>
    
      <item>
        <title>Alternating Direction Method of Multipliers (ADMM)</title>
        <description>&lt;p&gt;Consider minimizing &lt;script type=&quot;math/tex&quot;&gt; f({\bf x}) + g({\bf y}) &lt;/script&gt; subject to affine constraints &lt;script type=&quot;math/tex&quot;&gt; {\bf Ax} + {\bf By} = {\bf c} &lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The augmented Lagrangian&lt;/strong&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt; \mathcal{L}_\rho({\bf x}, {\bf y}, {\bf \lambda}) = f({\bf x}) + g({\bf y}) + \langle {\bf \lambda}, {\bf Ax} + {\bf By} - {\bf c} \rangle + \frac{\rho}{2} \| {\bf Ax} + {\bf By} - {\bf c} \|^2_2 &lt;/script&gt;&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;strong&gt;Idea&lt;/strong&gt;: perform block descent on &lt;script type=&quot;math/tex&quot;&gt;{\bf x}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;{\bf y}&lt;/script&gt; and then update multiplier vector &lt;script type=&quot;math/tex&quot;&gt;{\bf \lambda}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{align}
{\bf x}^{(t+1)}       &amp; \leftarrow \min_{\bf x} f({\bf x}) + \langle {\bf \lambda}, {\bf Ax} + {\bf By}^{(t)} - {\bf c} \rangle + \frac{\rho}{2} \| {\bf Ax} + {\bf By}^{(t)} - {\bf c} \|^2_2 \\
{\bf y}^{(t+1)}       &amp; \leftarrow \min_{\bf y} g({\bf y}) + \langle {\bf \lambda}, {\bf Ax}^{(t+1)} + {\bf By} - {\bf c} \rangle + \frac{\rho}{2} \| {\bf Ax}^{(t+1)} + {\bf By} - {\bf c} \|^2_2 \\
{\bf \lambda}^{(t+1)} &amp; \leftarrow {\bf \lambda}^{(t)} + \rho ({\bf Ax}^{(t+1)} + {\bf By}^{(t+1)} - {\bf c})
\end{align}
&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;example-fused-lasso&quot;&gt;Example: fused lasso&lt;/h2&gt;

&lt;p&gt;Fused lasso problem minimizes&lt;br /&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt; \frac{1}{2} \| {\bf y - X\beta} \|^2_2 + \mu \sum^{p-1}_{j=1} |\beta_{j+1} - \beta_j |&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Define &lt;script type=&quot;math/tex&quot;&gt;{\bf \gamma = D\beta}&lt;/script&gt;, where&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex; mode=display&quot;&gt;
D = \left(\begin{matrix}
1 &amp; -1 &amp; &amp;      &amp;   &amp; \\
  &amp;    &amp; \cdots &amp;   &amp; \\
  &amp;    &amp;        &amp; 1 &amp; -1 
\end{matrix}
\right)
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Then we minimize &lt;script type=&quot;math/tex&quot;&gt; \frac{1}{2} \| {\bf y} - {\bf X\beta} \|^2_2 + \mu \| \gamma \|_1 &lt;/script&gt; subject to &lt;script type=&quot;math/tex&quot;&gt; {\bf D\beta} = \gamma &lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Augmented Lagrangian is&lt;br /&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt; \mathcal{L}_\rho({\bf \beta}, {\bf \gamma}, {\bf \lambda}) = \frac{1}{2} \| {\bf y} - {\bf X\beta} \|^2_2 + \mu \| {\bf \gamma} \|_1 + {\bf \lambda}^T({\bf D\beta} - {\bf \gamma}) + \frac{\rho}{2} \| {\bf D\beta} - {\bf \gamma} \|^2_2 &lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;admm&quot;&gt;ADMM&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Update &lt;script type=&quot;math/tex&quot;&gt;{\bf \beta}&lt;/script&gt; is a smooth quadratic problem&lt;/li&gt;
  &lt;li&gt;Update &lt;script type=&quot;math/tex&quot;&gt;{\bf \gamma}&lt;/script&gt; is a separated lasso problem (elementwise thresholding)&lt;/li&gt;
  &lt;li&gt;Update multipliers&lt;br /&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;{\bf \lambda}^{(t+1)} \leftarrow {\bf \lambda}^{(t)} + \rho({\bf D\beta}^{(t)} - {\bf \gamma}^{(t)})&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Same algorithm applies to a general regularization matrix &lt;script type=&quot;math/tex&quot;&gt;{\bf D}&lt;/script&gt; (generalized lasso)&lt;/p&gt;

&lt;h2 id=&quot;remarks-on-admm&quot;&gt;Remarks on ADMM&lt;/h2&gt;

&lt;p&gt;Related algorithms&lt;/p&gt;

&lt;p&gt;Split Bregman iteration &lt;sup class=&quot;sidenote-number&quot;&gt;1&lt;/sup&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;sup class=&quot;sidenote-number&quot;&gt;1&lt;/sup&gt; Goldstein, T. and Osher, S. (2009). The split Bregman method for l1-regularized problems. SIAM J. Img. Sci., 2:323-343.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Dykstra’s alternating projection algorithm &lt;sup class=&quot;sidenote-number&quot;&gt;2&lt;/sup&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;sup class=&quot;sidenote-number&quot;&gt;2&lt;/sup&gt; Dykstra, R. L. (1983). An algorithm for restricted least squares regression. J. Amer. Statist. Assoc., 78(384):837-842.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Proximal point algorithm applied to the dual&lt;/p&gt;

&lt;p&gt;Numerous applications in statistics and machine learning: lasso, gen. lasso, graphical lasso, (overlapping) group lasso, …&lt;/p&gt;

&lt;p&gt;Embraces distributed computing for big data &lt;sup class=&quot;sidenote-number&quot;&gt;3&lt;/sup&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;sup class=&quot;sidenote-number&quot;&gt;3&lt;/sup&gt; Boyd, S., Parikh, N., Chu, E., Peleato, B., and Eckstein, J. (2011). Distributed optimization and statistical learning via the alternating direction method of multipliers. Found. Trends Mach. learn., 3(1):1-122.&lt;/span&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 18 Jul 2015 21:36:14 +0800</pubDate>
        <link>http://www.quxiaofeng.me/optimization/2015/07/18/alternating-direction-method-multipliers/</link>
        <guid isPermaLink="true">http://www.quxiaofeng.me/optimization/2015/07/18/alternating-direction-method-multipliers/</guid>
        
        
      </item>
    
      <item>
        <title>Augmented Lagrangian Method</title>
        <description>&lt;p&gt;Consider minimizing:&lt;/p&gt;

&lt;script type=&quot;math/tex&quot;&gt; f({\bf x}) &lt;/script&gt;
&lt;p&gt;subject to equality constraints &lt;script type=&quot;math/tex&quot;&gt; g_i({\bf x}) = 0 &lt;/script&gt; for &lt;script type=&quot;math/tex&quot;&gt;i=1, \ldots ,q&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Inequality constraints are ignored for simplicity&lt;/p&gt;

&lt;p&gt;Assume &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;g_i&lt;/script&gt; are smooth for simplicity&lt;/p&gt;

&lt;p&gt;At a constrained minimum, the Lagrange multiplier condition&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;  {\bf 0}=\nabla f({\bf x})+\sum^q_{i=1}\lambda_i\nabla g_i({\bf x})  &lt;/script&gt;

&lt;p&gt;holds provided &lt;script type=&quot;math/tex&quot;&gt;\nabla g_i({\bf x})&lt;/script&gt; are linearly independent&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;&lt;strong&gt;Augmented lagrangian&lt;/strong&gt; &lt;sup class=&quot;sidenote-number&quot;&gt;1&lt;/sup&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;sup class=&quot;sidenote-number&quot;&gt;1&lt;/sup&gt; 参考&lt;a href=&quot;http://www.stat.ncsu.edu/people/zhou/courses/st810/notes/lect24final.pdf&quot;&gt;http://www.stat.ncsu.edu/people/zhou/courses/st810/notes/lect24final.pdf&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}_\rho ({\bf x},{\bf \lambda}) = f({\bf x}) + \sum^q_{i=1}\lambda_i g_i({\bf x}) + \frac{\rho}{2}\sum^q_{i=1}g_i({\bf x})^2&lt;/script&gt;

&lt;p&gt;The penalty term &lt;script type=&quot;math/tex&quot;&gt;\frac{\rho}{2}\sum^q_{i=1}g_i({\bf x})^2&lt;/script&gt; punishes violations of the equality constraints &lt;script type=&quot;math/tex&quot;&gt;g_i({\bf \theta})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Optimize the Augmented Lagrangian and adjust &lt;script type=&quot;math/tex&quot;&gt;{\bf \lambda}&lt;/script&gt; in the hope of matching the true Lagrange multipliers&lt;/p&gt;

&lt;p&gt;For &lt;script type=&quot;math/tex&quot;&gt;\rho&lt;/script&gt; large enough (finite), the unconstrained minimizer of the augmented lagrangian coincides with the constrained solution of the original problem&lt;/p&gt;

&lt;p&gt;At convergence, the gradient &lt;script type=&quot;math/tex&quot;&gt;\rho g_i({\bf x})\nabla g_i({\bf x})&lt;/script&gt; vanishes and we recover the standard multiplier rule&lt;/p&gt;

&lt;h2 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h2&gt;

&lt;p&gt;Take &lt;script type=&quot;math/tex&quot;&gt;\rho&lt;/script&gt; initially large or gradually increase it; iterate&lt;/p&gt;

&lt;p&gt;Find the unconstrained minimum&lt;br /&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;{\bf x}^{(t+1)}\leftarrow \min_x\mathcal{L}_\rho ({\bf x},{\bf \lambda}^{(t)})&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Update the multiplier vector &lt;script type=&quot;math/tex&quot;&gt;{\bf \lambda}&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda^{(t+1)}_i \leftarrow \lambda^{(t)}_i + \rho g_i({\bf x}^{(t)}), \; i = 1, \ldots , q&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;intuition-for-updating-script-typemathtexbf-lambdascript&quot;&gt;Intuition for updating &lt;script type=&quot;math/tex&quot;&gt;{\bf \lambda}&lt;/script&gt;&lt;/h3&gt;

&lt;p&gt;If &lt;script type=&quot;math/tex&quot;&gt;{\bf x}^{(t)}&lt;/script&gt; is the unconstrained minimum of &lt;script type=&quot;math/tex&quot;&gt;\mathcal{L}({\bf x},{\bf \lambda})&lt;/script&gt;, then the stationary condition says&lt;br /&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{align}
{\bf 0} &amp;amp; = \nabla f({\bf x}^{(t))} + \sum^q_{i=1} \lambda^{(t)}_i \nabla g_i({\bf x}^{(t)}) + \rho \sum^q_{i=1} g_i({\bf x}^{(t)}) \nabla g_i({\bf x}^{(t)}) \\
&amp;amp; = \nabla f({\bf x}^{(t))} + \sum^q_{i=1} \left[ \lambda^{(t)}_i + \rho g_i({\bf x}^{(t)}) \right] \nabla g_i({\bf x}^{(t)})
\end{align}
&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;for-non-smooth-script-typemathtexfscript-replace-gradient-script-typemathtexnabla-fscript-by-sub-differential-script-typemathtexpartial-fscript&quot;&gt;For non-smooth &lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;, replace gradient &lt;script type=&quot;math/tex&quot;&gt;\nabla f&lt;/script&gt; by sub-differential &lt;script type=&quot;math/tex&quot;&gt;\partial f&lt;/script&gt;&lt;/h3&gt;

&lt;h2 id=&quot;example-basis-pursuit&quot;&gt;Example: basis pursuit&lt;/h2&gt;

&lt;p&gt;Basis pursuit problem seeks the sparsest solution subject to linear constraints&lt;br /&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{align}
\text{minimize    } &amp;amp; \|{\bf x}\|_1 \\
\text{subject to    } &amp;amp; {\bf Ax} = {\bf b}
\end{align}
&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Take &lt;script type=&quot;math/tex&quot;&gt;\rho&lt;/script&gt; initially large or gradually increase it; iterate according to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{align}
{\bf x}^{(t+1)}                 &amp; \leftarrow \min \|{\bf x}\| + \langle {\bf \lambda}^{(t)}, {\bf Ax} - {\bf b} \rangle + \frac{\rho}{2} \|{\bf Ax} - {\bf b}\|^2-2 \text{(lasso)} \\
{\bf \lambda}^{(t+1)} &amp; \leftarrow {\bf \lambda}^{(t)} + \rho \left( {\bf Ax}^{(t+1)} - {\bf b} \right) 
\end{align}
&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Converges in a finite (small) number of steps &lt;sup class=&quot;sidenote-number&quot;&gt;2&lt;/sup&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;sup class=&quot;sidenote-number&quot;&gt;2&lt;/sup&gt; Yin, W., Osher, S., Goldfarb, D., and Darbon, J. (2008). Bregman iterative algorithms for l&lt;sub&gt;1&lt;/sub&gt;-minimization with applications to compressed sensing. SIAM J. Imaging Sci., 1(1):143-168. Online: &lt;a href=&quot;http://www.caam.rice.edu/~wy1/paperfiles/Rice_CAAM_TR07-13.PDF&quot;&gt;http://www.caam.rice.edu/~wy1/paperfiles/Rice_CAAM_TR07-13.PDF&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;remarks&quot;&gt;Remarks&lt;/h2&gt;

&lt;p&gt;The augmented Lagrangian method dates back to 50s &lt;sup class=&quot;sidenote-number&quot;&gt;3&lt;/sup&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;sup class=&quot;sidenote-number&quot;&gt;3&lt;/sup&gt; Hestenes, M.R. (1969). Multiplier and gradient methods. J. Optimization Theory Appl., 4:303-320. &lt;br /&gt; Powell, M. J. D. (1969). A method for nonlinear constraints in minimization problems. In Optimization (Sympos., Univ. Keele, Keele, 1968), pages 283-298. Academic Press, London.&lt;/span&gt;&lt;br /&gt;
Monograph by Bertsekas&lt;sup class=&quot;sidenote-number&quot;&gt;4&lt;/sup&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;sup class=&quot;sidenote-number&quot;&gt;4&lt;/sup&gt; Bertsekas, D. P. (1982). Constrained Optimization and Lagrange Multiplier Methods. Computer Science and Applied Mathematics. Academic Press Inc. [Harcourt Brace Jovanovich Publishers], New York.&lt;/span&gt; provides a general treatment&lt;br /&gt;
Same as the Bregman iteration (Yin etal., 2008) proposed for basis pursuit (compressive sensing)&lt;br /&gt;
Equivalent to proximal print algorithm applied to the dual; can be accelerated (Nesterov)&lt;/p&gt;
</description>
        <pubDate>Fri, 17 Jul 2015 19:04:01 +0800</pubDate>
        <link>http://www.quxiaofeng.me/optimization/2015/07/17/augmented-lagrangian-method/</link>
        <guid isPermaLink="true">http://www.quxiaofeng.me/optimization/2015/07/17/augmented-lagrangian-method/</guid>
        
        
      </item>
    
      <item>
        <title>Introduction to Nonlinear Optimization</title>
        <description>&lt;p&gt;昨天晚上到今天，看完了一本之前一直看不完的书 《Introduction to Nonlinear Optimization》&lt;sup class=&quot;sidenote-number&quot;&gt;1&lt;/sup&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;sup class=&quot;sidenote-number&quot;&gt;1&lt;/sup&gt; 《Introduction to Nonlinear Optimization》 at 豆瓣: &lt;a href=&quot;http://book.douban.com/subject/26551626/&quot;&gt;http://book.douban.com/subject/26551626/&lt;/a&gt; and at Amazon: &lt;a href=&quot;http://www.amazon.com/Introduction-Nonlinear-Optimization-Algorithms-Applications/dp/1611973643/&quot;&gt;http://www.amazon.com/Introduction-Nonlinear-Optimization-Algorithms-Applications/dp/1611973643/&lt;/a&gt;&lt;/span&gt; by Amir Beck &lt;sup class=&quot;sidenote-number&quot;&gt;2&lt;/sup&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;sup class=&quot;sidenote-number&quot;&gt;2&lt;/sup&gt; Amir Beck is an associate professor at The Technion—Israel Institute of Technology： &lt;a href=&quot;http://iew3.technion.ac.il/Home/Users/becka.html&quot;&gt;http://iew3.technion.ac.il/Home/Users/becka.html&lt;/a&gt;&lt;/span&gt;。澄清了一些过去曾经误解的概念。MOS-SIAM Series on Optimization&lt;sup class=&quot;sidenote-number&quot;&gt;3&lt;/sup&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;sup class=&quot;sidenote-number&quot;&gt;3&lt;/sup&gt; MOS-SIAM Series on Optimization: &lt;a href=&quot;http://bookstore.siam.org/mos-siam-series-on-optimization/&quot;&gt;http://bookstore.siam.org/mos-siam-series-on-optimization/&lt;/a&gt;&lt;/span&gt; 一系列优化的书都不错。连着借了三本，希望以后都能好好读完。现在这本非线性优化暂时只是翻完了，习题都没做，感觉习题也都挺有用的。&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;table-of-content-&quot;&gt;Table of Content 目录&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; Mathematical Preliminaries 数学基础&lt;br /&gt;
&lt;strong&gt;2.&lt;/strong&gt; Optimality Conditions for Unconstrained Optimization 无约束优化问题的优化条件&lt;br /&gt;
&lt;strong&gt;3.&lt;/strong&gt; Least Squares 最小二乘问题&lt;br /&gt;
&lt;strong&gt;4.&lt;/strong&gt; The Gradient Method 梯度下降法&lt;br /&gt;
&lt;strong&gt;5.&lt;/strong&gt; Newton’s Method 牛顿法&lt;br /&gt;
&lt;strong&gt;6.&lt;/strong&gt; Convex Sets 凸集&lt;br /&gt;
&lt;strong&gt;7.&lt;/strong&gt; Convex Functions 凸函数&lt;br /&gt;
&lt;strong&gt;8.&lt;/strong&gt; Convex Optimization 凸优化&lt;br /&gt;
&lt;strong&gt;9.&lt;/strong&gt; Optimization over a Convex Set 凸集上的优化问题&lt;br /&gt;
&lt;strong&gt;10.&lt;/strong&gt; Optimality Conditions for Linearly Constrained Problems 线性约束问题的优化条件&lt;br /&gt;
&lt;strong&gt;11.&lt;/strong&gt; The KKT Conditions 库恩塔克条件&lt;br /&gt;
&lt;strong&gt;12.&lt;/strong&gt; Duality 对偶问题&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;第一章 数学基础&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Definition 1.1&lt;/strong&gt; 内积 inner product &lt;script type=&quot;math/tex&quot;&gt;\langle\cdot,\cdot\rangle:=\Re^n \times \Re^n \rightarrow \Re&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; 交换率 symmetry &lt;script type=&quot;math/tex&quot;&gt;\langle{\bf x},{\bf y}\rangle=\langle{\bf y},{\bf x}\rangle&lt;/script&gt; for any &lt;script type=&quot;math/tex&quot;&gt;{\bf x},{\bf y} \in \Re^n &lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; 分配率 additivity &lt;script type=&quot;math/tex&quot;&gt;\langle{\bf x},{\bf y}+{\bf z}\rangle=\langle{\bf x},{\bf y}\rangle+\langle{\bf x},{\bf z}\rangle&lt;/script&gt; for any &lt;script type=&quot;math/tex&quot;&gt;{\bf x},{\bf y},{\bf z} \in \Re^n &lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; 线性 homogeneity &lt;script type=&quot;math/tex&quot;&gt;\langle\lambda{\bf x},{\bf y}\rangle=\lambda\langle{\bf x},{\bf y}\rangle&lt;/script&gt; for any &lt;script type=&quot;math/tex&quot;&gt;\lambda\in \Re &lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;{\bf x},{\bf y} \in \Re^n&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4.&lt;/strong&gt; 正定 positive definiteness &lt;script type=&quot;math/tex&quot;&gt;\langle{\bf x},{\bf x}\rangle\ge0&lt;/script&gt; for any &lt;script type=&quot;math/tex&quot;&gt;{\bf x}\in \Re^n&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\langle{\bf x},{\bf x}\rangle=0&lt;/script&gt; if and only if &lt;script type=&quot;math/tex&quot;&gt;{\bf x}={\bf 0} \; \blacksquare&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 1.2&lt;/strong&gt; 最常见的内积就是点积 dot product&lt;br /&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\langle{\bf x},{\bf y}\rangle={\bf x}^T{\bf y}=\sum^n_{i=1}x_iy_i \text{ for any } {\bf x},{\bf y} \in \Re^n &lt;/script&gt;&lt;br /&gt;
点积是标准内积，当不明确说明时，默认内积就是点积。&lt;script type=&quot;math/tex&quot;&gt;\blacksquare&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 1.3&lt;/strong&gt; 加权点积是 &lt;script type=&quot;math/tex&quot;&gt;\Re^n &lt;/script&gt; 空间中另一个内积的例子，其中权重 &lt;script type=&quot;math/tex&quot;&gt;{\bf w}\in \Re^n_{++} &lt;/script&gt;。&lt;br /&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt; \langle {\bf x},{\bf y} \rangle_{\bf w} = \sum^n_{i=1}w_ix_iy_i \; \blacksquare&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition 1.4&lt;/strong&gt; Norm 范数 一个定义在实数向量集 &lt;script type=&quot;math/tex&quot;&gt;\Re^n &lt;/script&gt; 上的范数 &lt;script type=&quot;math/tex&quot;&gt;\|\cdot\| &lt;/script&gt; 是一个满足如下条件，形如 &lt;script type=&quot;math/tex&quot;&gt;\| \cdot \| : \Re^n \rightarrow \Re &lt;/script&gt; 的函数&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; 非负性 nonnegativity &lt;script type=&quot;math/tex&quot;&gt; \|{\bf x}\| \ge 0 \text{ for any } {\bf x} \in \Re^n \text{ and } \|{\bf x}\| = 0 \text{ if and only if } {\bf x} = {\bf 0} &lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; 正线性 positive homogeneity &lt;script type=&quot;math/tex&quot;&gt; |\lambda {\bf x}\| = |\lambda| \|{\bf x}\| \text{ for any } {\bf x} \in \Re^n \text{ and } \lambda \in \Re&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; 三角不等 triangle inequality  &lt;script type=&quot;math/tex&quot;&gt; \|{\bf x} + {\bf y}\| \le \|{\bf x}\| + \|{\bf y}\| \text{ for any } {\bf x},{\bf y} \in \Re^n&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;相应的，&lt;script type=&quot;math/tex&quot;&gt; p &lt;/script&gt; 范数定位为&lt;br /&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt; \ell_p \equiv \|{\bf x}\|_p \equiv \sqrt[p]{\sum^n_{i=1}|x_i|^p} \text{ , for }  p \ge 1 &lt;/script&gt;&lt;/p&gt;

&lt;p&gt;类似的，&lt;script type=&quot;math/tex&quot;&gt;\infty &lt;/script&gt; 范数定义为&lt;br /&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt; \|{\bf x}\|_{\infty} \equiv \max_{i=1,2,\ldots,n}|x_i|=\lim_{p \rightarrow \infty} \|{\bf x}\|_p \; \blacksquare&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 1.5&lt;/strong&gt; 柯西 - 施瓦茨不等式 Cauchy-Schwarz inequality &lt;script type=&quot;math/tex&quot;&gt; \text{ For any } {\bf x}, {\bf y} \in \Re^n,&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;|{\bf x}^T{\bf y}| \le \|{\bf x}\|_2 \cdot \|{\bf y}\|_2 &lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex&quot;&gt; \text{ Equality is satisfied if and only if } {\bf x} \text{ and } {\bf y} \text{ are linearly dependent.} &lt;/script&gt;

</description>
        <pubDate>Thu, 16 Jul 2015 05:22:17 +0800</pubDate>
        <link>http://www.quxiaofeng.me/optimization/2015/07/16/introduction-to-nonlinear-optimization/</link>
        <guid isPermaLink="true">http://www.quxiaofeng.me/optimization/2015/07/16/introduction-to-nonlinear-optimization/</guid>
        
        
      </item>
    
      <item>
        <title>稀疏编码的优化问题</title>
        <description>&lt;p&gt;稀疏编码问题：  &lt;script type=&quot;math/tex&quot;&gt; \arg\min_x f(x)\left\|y-Ax\right\|^2+ \lambda\|x\|_1 &lt;/script&gt;&lt;/p&gt;

&lt;p&gt;用 alternating minimization (ADM)&lt;/p&gt;

&lt;p&gt;Primal-dual 算法&lt;/p&gt;

&lt;p&gt;Soft threshold&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;字典学习 K-SVD&lt;/p&gt;

&lt;p&gt;看完 K-SVD 之后，图像分类: Discriminative K-SVD for Dictionary Learning in Face Recognition (CVPR)，Label Consistent K-SVD Learning a Discriminative Dictionary for Recognition （TPAMI）；结构化稀疏: Robust Classiﬁcation using Structured Sparse Representation （CVPR）；低秩: Learning Structured Low-rank Representations for Image Classiﬁcation （CVPR）。&lt;/p&gt;

&lt;p&gt;关于稀疏表示的已有算法的分析: A survey of sparse representation: algorithms and applications&lt;/p&gt;

&lt;p&gt;参考林倞老师&lt;sup class=&quot;sidenote-number&quot;&gt;1&lt;/sup&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;sup class=&quot;sidenote-number&quot;&gt;1&lt;/sup&gt; &lt;a href=&quot;http://ss.sysu.edu.cn/~ll/&quot;&gt;林倞教授 中山大学计算机科学&lt;/a&gt;&lt;/span&gt; NIPS, ACML 和 ML 一系列工作的报告。基本上 ADM 及其扩展都介绍到了。&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;newthought&quot;&gt;迭代优化&lt;/span&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^p\left\|y^{(i)}-A\cdot x^{(i)}\right\|_2^2 + \sum_{i=1}^p S\left(x^{(i)}\right) &lt;/script&gt;

&lt;p&gt;凸的稀疏模型求基于 proximal operator 的 alternating minimization。参考 boyd 的文章。&lt;/p&gt;

&lt;p&gt;An ADMM Solution to the Sparse Coding Problem: &lt;sup class=&quot;sidenote-number&quot;&gt;2&lt;/sup&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;sup class=&quot;sidenote-number&quot;&gt;2&lt;/sup&gt; &lt;a href=&quot;http://stanford.edu/class/ee364b/projects/2011projects/reports/bhaskar_zou.pdf&quot;&gt;http://stanford.edu/class/ee364b/projects/2011projects/reports/bhaskar_zou.pdf&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.eecs.berkeley.edu/~yang/software/l1benchmark/&quot;&gt;http://www.eecs.berkeley.edu/~yang/software/l1benchmark/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;这篇文章 &lt;sup class=&quot;sidenote-number&quot;&gt;3&lt;/sup&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;sup class=&quot;sidenote-number&quot;&gt;3&lt;/sup&gt; &lt;a href=&quot;http://web.stanford.edu/~boyd/papers/prox_algs.html&quot;&gt;http://web.stanford.edu/~boyd/papers/prox_algs.html&lt;/a&gt;&lt;/span&gt; 从 proximal operator 角度讲解，更加深入一点。&lt;/p&gt;

&lt;p&gt;南京大学何炳生老师的课程 &lt;sup class=&quot;sidenote-number&quot;&gt;4&lt;/sup&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;sup class=&quot;sidenote-number&quot;&gt;4&lt;/sup&gt; &lt;a href=&quot;http://math.nju.edu.cn/~hebma/&quot;&gt;http://math.nju.edu.cn/~hebma/&lt;/a&gt;&lt;/span&gt;，讲得比较深入。&lt;/p&gt;

&lt;p&gt;biconvex 问题，基本是个保证收敛的算法效果就还行。&lt;/p&gt;
</description>
        <pubDate>Thu, 21 May 2015 08:33:45 +0800</pubDate>
        <link>http://www.quxiaofeng.me/optimization/2015/05/21/solving-sparse/</link>
        <guid isPermaLink="true">http://www.quxiaofeng.me/optimization/2015/05/21/solving-sparse/</guid>
        
        
      </item>
    
      <item>
        <title>几个基本优化问题</title>
        <description>&lt;p&gt;可以用 ALM&lt;sup class=&quot;sidenote-number&quot;&gt;1&lt;/sup&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;sup class=&quot;sidenote-number&quot;&gt;1&lt;/sup&gt; Augmented Lagrange Multiplier 增广拉格朗日乘子法&lt;/span&gt;, LP&lt;sup class=&quot;sidenote-number&quot;&gt;2&lt;/sup&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;sup class=&quot;sidenote-number&quot;&gt;2&lt;/sup&gt; Linear Programming 线性规划&lt;/span&gt; 和 IRLS &lt;sup class=&quot;sidenote-number&quot;&gt;3&lt;/sup&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;sup class=&quot;sidenote-number&quot;&gt;3&lt;/sup&gt; &lt;a href=&quot;http://www.robotics.stanford.edu/~ang/papers/aaai06-efficientL1logisticregression.pdf&quot;&gt;Iteratively Reweighted Least Squares&lt;/a&gt;&lt;/span&gt; 求解的四种基本优化问题&lt;/p&gt;

&lt;h3 id=&quot;question-1&quot;&gt;Question 1&lt;/h3&gt;

&lt;p&gt;&lt;span class=&quot;newthought&quot;&gt;least entropy &amp;amp; error correction&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Question 1: &lt;script type=&quot;math/tex&quot;&gt; \arg \min \|x\|_1 + \|e\|_1 &lt;/script&gt; subj. to &lt;script type=&quot;math/tex&quot;&gt; y=Ax+e &lt;/script&gt;&lt;/p&gt;

&lt;p&gt;标准 linear programming&lt;/p&gt;

&lt;p&gt;鲁棒的 &lt;em&gt;SRC&lt;/em&gt;&lt;sup class=&quot;sidenote-number&quot;&gt;4&lt;/sup&gt;&lt;span class=&quot;sidenote&quot;&gt;&lt;sup class=&quot;sidenote-number&quot;&gt;4&lt;/sup&gt; 参见 &lt;a href=&quot;http://research.microsoft.com/pubs/132810/PAMI-Face.pdf&quot;&gt;http://research.microsoft.com/pubs/132810/PAMI-Face.pdf&lt;/a&gt; and &lt;a href=&quot;http://perception.csl.illinois.edu/recognition/Home.html&quot;&gt;Face Recognition via Sparse Representation&lt;/a&gt;&lt;/span&gt;。使用单位矩阵作为遮挡字典，用标准形式求解。&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;question-2&quot;&gt;Question 2&lt;/h3&gt;

&lt;p&gt;&lt;span class=&quot;newthought&quot;&gt;least energy &amp;amp; error correction&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Question 2: &lt;script type=&quot;math/tex&quot;&gt; \arg\min \|x\|_2 + \|e\|_1 &lt;/script&gt; subj. to &lt;script type=&quot;math/tex&quot;&gt; y=Ax+e &lt;/script&gt;&lt;/p&gt;

&lt;p&gt;鲁棒的 CRC&lt;/p&gt;

&lt;h3 id=&quot;question-3&quot;&gt;Question 3&lt;/h3&gt;

&lt;p&gt;&lt;span class=&quot;newthought&quot;&gt;sparse regression with noise - lasso&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Question 3:  &lt;script type=&quot;math/tex&quot;&gt; \arg\min \|x\|_1 + \|e\|_2 &lt;/script&gt; subj. to &lt;script type=&quot;math/tex&quot;&gt; y=Ax+e &lt;/script&gt;&lt;/p&gt;

&lt;p&gt;标准 lasso 问题&lt;/p&gt;

&lt;p&gt;标准的 SRC&lt;/p&gt;

&lt;h3 id=&quot;question-4&quot;&gt;Question 4&lt;/h3&gt;

&lt;p&gt;&lt;span class=&quot;newthought&quot;&gt;least energy with noise&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Question 4: &lt;script type=&quot;math/tex&quot;&gt; \arg\min \|x\|_2 + \|e\|_2 &lt;/script&gt; subj. to &lt;script type=&quot;math/tex&quot;&gt; y=Ax+e &lt;/script&gt;&lt;/p&gt;

&lt;p&gt;极小最小二乘解，可求广义逆。&lt;/p&gt;

&lt;p&gt;CRC，用二范数约束表示系数，有解析解。&lt;/p&gt;
</description>
        <pubDate>Thu, 21 May 2015 06:53:20 +0800</pubDate>
        <link>http://www.quxiaofeng.me/optimization/2015/05/21/optimization-problems/</link>
        <guid isPermaLink="true">http://www.quxiaofeng.me/optimization/2015/05/21/optimization-problems/</guid>
        
        
      </item>
    
  </channel>
</rss>

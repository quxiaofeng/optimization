<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>The Expectation Maximization Algorithm and Finite Mixture Models | OPTIMIZATIONS for Machine Learning</title>
  <meta name="description" content="期望最大化算法和有限混合模型概念主要来自于一次跟师弟的讨论。师弟提到 Expectation Maximization Algorithm (EM 算法) 方面的专家 Prof. Geoffrey John McLachlan 的一篇早期的论文就完全涵盖了多个最新顶级期刊论文的思想。就跟着追到该教授的主页，找到这...">


  <link rel="stylesheet" href="//www.optimizations.ml/css/tufte.css">


  <!-- Google Fonts loaded here -->
  <link href='//fonts.useso.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>

  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <!-- Load up KaTeX script if needed ... specify in /_data/options.yml file-->
  
  <script src="//code.jquery.com/jquery-1.11.1.min.js"></script>
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.js"></script>
  

  <link rel="canonical" href="http://www.optimizations.ml/2015/07/24/em-method/">
  <link rel="alternate" type="application/rss+xml" title="OPTIMIZATIONS for Machine Learning" href="http://www.optimizations.ml/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
		<a href="//www.optimizations.ml/"><img class="badge" src="//www.optimizations.ml/assets/img/badge_op_ml.png" alt="OP"></a>
		<a href="//www.optimizations.ml/">Home</a>
		<a href="//www.optimizations.ml/about">About</a>
	</nav>
</header>

    <article>
      <h1>The Expectation Maximization Algorithm and Finite Mixture Models</h1>
<p>July 24, 2015</p>


<p>期望最大化算法和有限混合模型</p>

<p>概念主要来自于一次跟师弟的讨论。师弟提到 Expectation Maximization Algorithm (EM 算法) 方面的专家 <a href="http://www.maths.uq.edu.au/~gjm/">Prof. Geoffrey John McLachlan</a> 的一篇早期的论文就完全涵盖了多个最新顶级期刊论文的思想。就跟着追到该教授的<a href="http://www.maths.uq.edu.au/~gjm/">主页</a>，找到这两本书：<a href="http://book.douban.com/subject/2857809/">Finite Mixture Models (有限混合模型)</a> 和 <a href="http://book.douban.com/subject/3554292/">The EM Algorithm and Extensions (期望最大化算法及其拓展)</a>。</p>

<!--more-->

<h2 id="prof-geoffrey-john-mclachlanhttpwwwmathsuqeduaugjm-"><a href="http://www.maths.uq.edu.au/~gjm/">Prof. Geoffrey John McLachlan</a> 近期相关论文</h2>

<ul>
  <li>McLachlan, G.J. and Ng, S.K. (2009). <a href="http://www.maths.uq.edu.au/~gjm/mn_em.pdf">The EM Algorithm</a>. In The Top-Ten Algorithms in Data Mining, X. Wu and V. Kumar (Eds.). Boca Raton, Florida: Chapman &amp; Hall/CRC, pp. 93-115.</li>
  <li>McLachlan, G.J., Ng, S.K., and Wang, K. (2010). <a href="http://www.maths.uq.edu.au/~gjm/mnw_cladag07.pdf">Clustering of high-dimensional and correlated data</a>. In Studies in Classification, Data Analysis, and Knowledge Organization: Data Analysis and Classification, C. Lauro, F. Palumbo, and M. Greenacre (Eds.). Berlin: Springer-Verlag, pp. 3-11.</li>
  <li>McLachlan, G.J. and Baek, J. (2010). <a href="http://www.maths.uq.edu.au/~gjm/mb_gfkl08.pdf">Clustering of high-dimensional data via finite mixture models</a>. In Advances in Data Analysis, Data Handling and Business Intelligence, A. Fink, B. Lausen, W. Seidel, and A. Ultsch (Eds.). Berlin: Springer-Verlag, pp. 33-44.</li>
  <li>Baek, J., McLachlan, G.J., and Flack, L. (2010). <a href="http://www.maths.uq.edu.au/~gjm/bmf_pami09.pdf">Mixtures of factor analyzers with common factor loadings: applications to the clustering and visualisation of high-dimensional data</a>. IEEE Transactions on Pattern Analysis and Machine Intelligence 32, 1298-1309.</li>
  <li>Nikulin, V., Huang, T.-H., Ng, S.K., Rathnayake, S.I., and McLachlan, G.J. (2011). <a href="http://www.maths.uq.edu.au/~gjm/nhnrm_spl11.pdf">A very fast algorithm for matrix factorization</a>. Statistics &amp; Probability Letters 81, 773-782.</li>
  <li>Lee, S.X. and McLachlan, G.J. (2011). <a href="http://arxiv.org/abs/1109.4706">On the fitting of mixtures of multivariate skew t-distributions via the EM algorithm</a>. Preprint arXiv:1109.4706v2.</li>
  <li>Lee, S. and McLachlan, G.J. (2014). <a href="http://www.maths.uq.edu.au/~gjm/lm_sc2014a.pdf">Finite mixtures of multivariate skew t-distributions: some recent and new results</a>. Statistics and Computing 24, 181-202. See also amended version with corrections.</li>
  <li>Lin, T.-I., McLachlan, G.J., and Lee, S.X. (2016). <a href="http://arxiv.org/abs/1307.1748">Extending mixtures of factor models using the restricted multivariate skew-normal distribution</a>. Journal of Multivariate Analysis. To appear. Preprint arXiv.1307.1748.</li>
  <li>McLachlan, G.J. and Lee, S.X. (2014). <a href="http://arxiv.org/abs/1404.1733">Comment on “Comparing two formulations of skew distributions with special reference to model-based clustering” by A. Azzalini, R. Browne, M. Genton, and P. McNicholas</a>. Preprint arXiv:1404.1733.</li>
  <li>Lee, S.X., McLachlan, G.J., and Pyne, S. (2014). <a href="http://arxiv.org/abs/1411.2820">Supervised classification of flow cytometric samples via the joint clustering and matching (JCM) procedure</a>. Preprint arXiv:1411.0685.</li>
</ul>

<h2 id="section">高斯混合模型</h2>

<p>在应用领域，更多提及的是高斯混合模型 (GMM, Gaussian Mixture Model)。最简单的是单高斯模型。在单高斯模型中，假设多维变量符合高斯分布。高斯分布从空间上观察，在二维情况，近似于椭圆；在三维情况，近似于椭球。当单高斯分布不足以表达高维随机变量的分布时，用多个高斯分布的合成来近似该高维随机变量的分布。可以类比小波分解，使用足够多的高斯分布，可以拟合任意高维数据分布。特别考虑到，独立同分布 (i.i.d., independent identically distributed) 随机变量，当数据足够多时，近似服从高斯分布。</p>

<p>这里 GMM 跟 K-means 算法在很多方面近似。K-means 算法是从一组样本中选择其中一个最近邻样本作为聚类中心。GMM 中是根据一组数据按照高斯分布计算中心与方差，然后为每一个数据计算属于该高斯分布的概率。GMM 可以看成是一种柔性、概率的 K-means。K-means 可以看成是鉴别/分类方法 (identification/classification)；对应的 GMM 可以看成是验证方法 (veriication)。K-means 把每一个样本分配到 K 个聚类中心中最近的一个；GMM 为每一个样本计算其属于 K 个不同的高斯分布的概率。</p>

<p>给定样本及类别信息，可以通过极大似然估计 (ML, Maximum Likelihood) 算法求解 GMM 参数。当样本类别未知时，使用各样本符合分布的概率，期望最大化算法 (EM)。当微小概率连乘时，可以通过取对数的方式，提高数值计算的稳定性。</p>




    </article>
    <span class="print-footer">The Expectation Maximization Algorithm and Finite Mixture Models - July 24, 2015 - QU Xiaofeng</span>
    <footer>
  <ul class="footer-links group">
    <li><a href="mailto:xiaofeng.qu.hk@ieee.org"><span class="icon-mail"></span></a></li>
    
      <li>
        <a href="//www.twitter.com/quxiaofeng"><span class="icon-twitter"></span></a>
      </li>
    
      <li>
        <a href="//www.facebook.com/quxiaofeng"><span class="icon-facebook"></span></a>
      </li>
    
      <li>
        <a href="//github.com/quxiaofeng"><span class="icon-github"></span></a>
      </li>
    
      <li>
        <a href="//www.quxiaofeng.me/optimization/feed"><span class="icon-feed"></span></a>
      </li>
    
  </ul>
<div class="credits">
<span>&copy; 2016 &nbsp;&nbsp;QU XIAOFENG</span></br>
<span> Themed by <a href="//github.com/clayh53/tufte-jekyll">Tufte</a> &nbsp;&nbsp; Hosted in <a href="//www.github.com">GitHub</a> &nbsp;&nbsp; Powered by <a href="//jekyllrb.com">Jekyll</a></span>
</div>
</footer>

<!-- Load up KaTeX script if needed ... specify in /_data/options.yml file-->

<script src="//www.optimizations.ml/assets/js/katex_render.min.js"></script>

  </body>
</html>

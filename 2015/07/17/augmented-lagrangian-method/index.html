<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Augmented Lagrangian Method | OPTIMIZATIONS for Machine Learning</title>
  <meta name="description" content="Consider minimizing:​ subject to equality constraints ​ for ​Inequality constraints are ignored for simplicityAssume ​ and ​ are smooth for simplicityAt a co...">


  <link rel="stylesheet" href="//www.optimizations.ml/css/tufte.css">


  <!-- Google Fonts loaded here -->
  <link href='//fonts.useso.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>

  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <link rel="canonical" href="http://www.optimizations.ml/2015/07/17/augmented-lagrangian-method/">
  <link rel="alternate" type="application/rss+xml" title="OPTIMIZATIONS for Machine Learning" href="http://www.optimizations.ml/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
    <nav class="group">
		<a href="//www.optimizations.ml/"><img class="badge" src="//www.optimizations.ml/assets/img/badge_op_ml.png" alt="OP"></a>
		<a href="//www.optimizations.ml/">Home</a>
		<a href="//www.optimizations.ml/about">About</a>
	</nav>
</header>

    <article>
      <h1>Augmented Lagrangian Method</h1>
<p>July 17, 2015</p>


<p>Consider minimizing:</p>

<p>​<script type="math/tex"> f({\bf x}) </script> subject to equality constraints ​<script type="math/tex"> g_i({\bf x}) = 0 </script> for ​<script type="math/tex">i=1, \ldots ,q</script></p>

<p>Inequality constraints are ignored for simplicity</p>

<p>Assume ​<script type="math/tex">f</script> and ​<script type="math/tex">g_i</script> are smooth for simplicity</p>

<p>At a constrained minimum, the Lagrange multiplier condition</p>

<p>​<script type="math/tex; mode=display">  {\bf 0}=\nabla f({\bf x})+\sum^q_{i=1}\lambda_i\nabla g_i({\bf x})  </script></p>

<p>holds provided ​<script type="math/tex">\nabla g_i({\bf x})</script> are linearly independent</p>

<!--more-->

<p><strong>Augmented lagrangian</strong> <sup class="sidenote-number">1</sup><span class="sidenote"><sup class="sidenote-number">1</sup> 参考<a href="http://www.stat.ncsu.edu/people/zhou/courses/st810/notes/lect24final.pdf">http://www.stat.ncsu.edu/people/zhou/courses/st810/notes/lect24final.pdf</a></span></p>

<p>​<script type="math/tex; mode=display">\mathcal{L}_\rho ({\bf x},{\bf \lambda}) = f({\bf x}) + \sum^q_{i=1}\lambda_i g_i({\bf x}) + \frac{\rho}{2}\sum^q_{i=1}g_i({\bf x})^2</script></p>

<p>The penalty term ​<script type="math/tex">\frac{\rho}{2}\sum^q_{i=1}g_i({\bf x})^2</script> punishes violations of the equality constraints ​<script type="math/tex">g_i({\bf \theta})</script></p>

<p>Optimize the Augmented Lagrangian and adjust ​<script type="math/tex">{\bf \lambda}</script> in the hope of matching the true Lagrange multipliers</p>

<p>For ​<script type="math/tex">\rho</script> large enough (finite), the unconstrained minimizer of the augmented lagrangian coincides with the constrained solution of the original problem</p>

<p>At convergence, the gradient ​<script type="math/tex">\rho g_i({\bf x})\nabla g_i({\bf x})</script> vanishes and we recover the standard multiplier rule</p>

<h2 id="algorithm">Algorithm</h2>

<p>Take ​<script type="math/tex">\rho</script> initially large or gradually increase it; iterate</p>

<p>Find the unconstrained minimum<br />
​<script type="math/tex; mode=display">{\bf x}^{(t+1)}\leftarrow \min_x\mathcal{L}_\rho ({\bf x},{\bf \lambda}^{(t)})</script></p>

<p>Update the multiplier vector ​<script type="math/tex">{\bf \lambda}</script><br />
​<script type="math/tex; mode=display">\lambda^{(t+1)}_i \leftarrow \lambda^{(t)}_i + \rho g_i({\bf x}^{(t)}), \; i = 1, \ldots , q</script></p>

<h3 id="intuition-for-updating-8203script-typemathtexbf-lambdascript">Intuition for updating ​<script type="math/tex">{\bf \lambda}</script></h3>

<p>If ​<script type="math/tex">{\bf x}^{(t)}</script> is the unconstrained minimum of ​<script type="math/tex">\mathcal{L}({\bf x},{\bf \lambda})</script>, then the stationary condition says</p>

<p>​<script type="math/tex; mode=display"> \begin{eqnarray*}
{\bf 0} &amp; = \nabla f({\bf x}^{(t)}) + \sum^q_{i=1} \lambda^{(t)}_i \nabla g_i({\bf x}^{(t)}) + \rho \sum^q_{i=1} g_i({\bf x}^{(t)}) \nabla g_i({\bf x}^{(t)}) \\
        &amp; = \nabla f({\bf x}^{(t)}) + \sum^q_{i=1} \left[ \lambda^{(t)}_i + \rho g_i({\bf x}^{(t)}) \right] \nabla g_i({\bf x}^{(t)})
\end{eqnarray*}</script></p>

<h3 id="for-non-smooth-8203script-typemathtexfscript-replace-gradient-8203script-typemathtexnabla-fscript-by-sub-differential-8203script-typemathtexpartial-fscript">For non-smooth ​<script type="math/tex">f</script>, replace gradient ​<script type="math/tex">\nabla f</script> by sub-differential ​<script type="math/tex">\partial f</script></h3>

<h2 id="example-basis-pursuit">Example: basis pursuit</h2>

<p>Basis pursuit problem seeks the sparsest solution subject to linear constraints<br />
​<script type="math/tex; mode=display">
\begin{align}
\text{minimize    } &amp; \|{\bf x}\|_1 \\
\text{subject to    } &amp; {\bf Ax} = {\bf b}
\end{align}
</script></p>

<p>Take ​<script type="math/tex">\rho</script> initially large or gradually increase it; iterate according to</p>

<p>​<script type="math/tex; mode=display"> \begin{eqnarray*}
{\bf x}^{(t+1)}       &amp; \leftarrow \min \|{\bf x}\| + \langle {\bf \lambda}^{(t)}, {\bf Ax} - {\bf b} \rangle + \frac{\rho}{2} \|{\bf Ax} - {\bf b}\|^2-2 \text{(lasso)} \\
{\bf \lambda}^{(t+1)} &amp; \leftarrow {\bf \lambda}^{(t)} + \rho \left( {\bf Ax}^{(t+1)} - {\bf b} \right)
\end{eqnarray*}</script></p>

<ul>
  <li>Converges in a finite (small) number of steps <sup class="sidenote-number">2</sup><span class="sidenote"><sup class="sidenote-number">2</sup> Yin, W., Osher, S., Goldfarb, D., and Darbon, J. (2008). Bregman iterative algorithms for l<sub>1</sub>-minimization with applications to compressed sensing. SIAM J. Imaging Sci., 1(1):143-168. Online: <a href="http://www.caam.rice.edu/~wy1/paperfiles/Rice_CAAM_TR07-13.PDF">http://www.caam.rice.edu/~wy1/paperfiles/Rice_CAAM_TR07-13.PDF</a></span></li>
</ul>

<h2 id="remarks">Remarks</h2>

<p>The augmented Lagrangian method dates back to 50s <sup class="sidenote-number">3</sup><span class="sidenote"><sup class="sidenote-number">3</sup> Hestenes, M.R. (1969). Multiplier and gradient methods. J. Optimization Theory Appl., 4:303-320. <br /> Powell, M. J. D. (1969). A method for nonlinear constraints in minimization problems. In Optimization (Sympos., Univ. Keele, Keele, 1968), pages 283-298. Academic Press, London.</span><br />
Monograph by Bertsekas<sup class="sidenote-number">4</sup><span class="sidenote"><sup class="sidenote-number">4</sup> Bertsekas, D. P. (1982). Constrained Optimization and Lagrange Multiplier Methods. Computer Science and Applied Mathematics. Academic Press Inc. [Harcourt Brace Jovanovich Publishers], New York.</span> provides a general treatment<br />
Same as the Bregman iteration (Yin etal., 2008) proposed for basis pursuit (compressive sensing)<br />
Equivalent to proximal print algorithm applied to the dual; can be accelerated (Nesterov)</p>



    </article>
    <span class="print-footer">Augmented Lagrangian Method - July 17, 2015 - QU Xiaofeng</span>
    <footer>
  <ul class="footer-links group">
    <li><a href="mailto:xiaofeng.qu.hk@ieee.org"><span class="icon-mail"></span></a></li>
    
      <li>
        <a href="//www.twitter.com/quxiaofeng"><span class="icon-twitter"></span></a>
      </li>
    
      <li>
        <a href="//www.facebook.com/quxiaofeng"><span class="icon-facebook"></span></a>
      </li>
    
      <li>
        <a href="//github.com/quxiaofeng"><span class="icon-github"></span></a>
      </li>
    
      <li>
        <a href="//www.quxiaofeng.me/optimization/feed"><span class="icon-feed"></span></a>
      </li>
    
  </ul>
<div class="credits">
<span>&copy; 2015 &nbsp;&nbsp;QU XIAOFENG</span></br>
<span> Themed by <a href="//github.com/clayh53/tufte-jekyll">Tufte</a> &nbsp;&nbsp; Hosted in <a href="//www.github.com">GitHub</a> &nbsp;&nbsp; Powered by <a href="//jekyllrb.com">Jekyll</a></span>
</div>
</footer>
  </body>
</html>
